{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install torchview","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:03.803386Z","iopub.execute_input":"2023-06-28T19:26:03.803722Z","iopub.status.idle":"2023-06-28T19:26:14.052983Z","shell.execute_reply.started":"2023-06-28T19:26:03.803699Z","shell.execute_reply":"2023-06-28T19:26:14.051788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport io\nimport numpy as np\nimport pandas as pd \nfrom pathlib import Path\nimport pickle\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom tqdm.auto import tqdm\n\nimport torchinfo\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport torchvision\nfrom torchvision import datasets, models\nfrom torchvision.transforms import ToTensor, Compose\nfrom torchvision.transforms.functional import to_pil_image, pil_to_tensor\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport PIL\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colormaps","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:14.055088Z","iopub.execute_input":"2023-06-28T19:26:14.055416Z","iopub.status.idle":"2023-06-28T19:26:17.284385Z","shell.execute_reply.started":"2023-06-28T19:26:14.055391Z","shell.execute_reply":"2023-06-28T19:26:17.283408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.285793Z","iopub.execute_input":"2023-06-28T19:26:17.286427Z","iopub.status.idle":"2023-06-28T19:26:17.293994Z","shell.execute_reply.started":"2023-06-28T19:26:17.286393Z","shell.execute_reply":"2023-06-28T19:26:17.292964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"art_dataset_path = Path(\"/kaggle/input/real-ai-art/Real_AI_SD_LD_Dataset/Real_AI_SD_LD_Dataset\")","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.295138Z","iopub.execute_input":"2023-06-28T19:26:17.295492Z","iopub.status.idle":"2023-06-28T19:26:17.304292Z","shell.execute_reply.started":"2023-06-28T19:26:17.295455Z","shell.execute_reply":"2023-06-28T19:26:17.303282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_art_dir = art_dataset_path / \"train\"\nvalid_art_dir = art_dataset_path / \"test\" # Change later when validation set is available\ntest_art_dir = art_dataset_path / \"test\"\n\ntrain_art_csv_dir = \"/kaggle/input/real-ai-art/train_art_col_feat.csv\"\nvalid_art_csv_dir = \"/kaggle/input/real-ai-art/test_art_col_feat.csv\" # Change later when validation set is available\ntest_art_csv_dir = \"/kaggle/input/real-ai-art/test_art_col_feat.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.308020Z","iopub.execute_input":"2023-06-28T19:26:17.308323Z","iopub.status.idle":"2023-06-28T19:26:17.315653Z","shell.execute_reply.started":"2023-06-28T19:26:17.308300Z","shell.execute_reply":"2023-06-28T19:26:17.314838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"art_class_labels = [\n    'AI_LD_art_nouveau',\n    'AI_LD_baroque',\n    'AI_LD_expressionism',\n    'AI_LD_impressionism',\n    'AI_LD_post_impressionism',\n    'AI_LD_realism',\n    'AI_LD_renaissance',\n    'AI_LD_romanticism',\n    'AI_LD_surrealism',\n    'AI_LD_ukiyo-e',\n    'AI_SD_art_nouveau',\n    'AI_SD_baroque',\n    'AI_SD_expressionism',\n    'AI_SD_impressionism',\n    'AI_SD_post_impressionism',\n    'AI_SD_realism',\n    'AI_SD_renaissance',\n    'AI_SD_romanticism',\n    'AI_SD_surrealism',\n    'AI_SD_ukiyo-e',\n    'art_nouveau',\n    'baroque',\n    'expressionism',\n    'impressionism',\n    'post_impressionism',\n    'realism',\n    'renaissance',\n    'romanticism',\n    'surrealism',\n    'ukiyo_e']","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.316725Z","iopub.execute_input":"2023-06-28T19:26:17.316958Z","iopub.status.idle":"2023-06-28T19:26:17.326155Z","shell.execute_reply.started":"2023-06-28T19:26:17.316934Z","shell.execute_reply":"2023-06-28T19:26:17.325410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Creation","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_img_colour_features(image):\n    rgb_img_features = np.array(image.resize((1,1))).squeeze()\n    hsv_img_features = np.array(image.convert('HSV').resize((1,1))).squeeze()\n    cmyk_img_features = np.array(image.convert('CMYK').resize((1,1))).squeeze()[:-1]\n    \n    return np.concatenate((rgb_img_features, hsv_img_features, cmyk_img_features)) # R G B H S V C M Y","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.327178Z","iopub.execute_input":"2023-06-28T19:26:17.327409Z","iopub.status.idle":"2023-06-28T19:26:17.337608Z","shell.execute_reply.started":"2023-06-28T19:26:17.327388Z","shell.execute_reply":"2023-06-28T19:26:17.336407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_img = PIL.Image.open(\"/kaggle/input/artbench-10-256px/artbench-10-imagefolder-split/artbench-10-imagefolder-split/test/art_nouveau/achille-beltrame_fly-of-gabriele-dannunzio-over-trieste-1915.jpg\")\n#test_img = PIL.Image.open(\"/kaggle/input/artbench-10-256px/artbench-10-imagefolder-split/artbench-10-imagefolder-split/test/romanticism/albert-bierstadt_sentinel-falls-and-cathedral-peaks-in-the-yosemite-valley-1864.jpg\")\n#test_img = PIL.Image.open(\"/kaggle/input/artbench-10-256px/artbench-10-imagefolder-split/artbench-10-imagefolder-split/test/realism/abbott-handerson-thayer_azores.jpg\")\ntest_img = PIL.Image.open(\"/kaggle/input/artbench-10-256px/artbench-10-imagefolder-split/artbench-10-imagefolder-split/test/ukiyo_e/chokosai-eisho_99.jpg\")","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.338492Z","iopub.execute_input":"2023-06-28T19:26:17.338700Z","iopub.status.idle":"2023-06-28T19:26:17.374123Z","shell.execute_reply.started":"2023-06-28T19:26:17.338682Z","shell.execute_reply":"2023-06-28T19:26:17.373470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img.filename","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.375235Z","iopub.execute_input":"2023-06-28T19:26:17.375490Z","iopub.status.idle":"2023-06-28T19:26:17.380615Z","shell.execute_reply.started":"2023-06-28T19:26:17.375469Z","shell.execute_reply":"2023-06-28T19:26:17.379950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_col_features = get_img_colour_features(test_img)\nimg_col_features","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.381281Z","iopub.execute_input":"2023-06-28T19:26:17.381498Z","iopub.status.idle":"2023-06-28T19:26:17.402892Z","shell.execute_reply.started":"2023-06-28T19:26:17.381479Z","shell.execute_reply":"2023-06-28T19:26:17.401798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_col_features(dir_img_dataset):\n    count = 0\n    img_features = []\n\n    for sub_dir, dirs, img_files in os.walk(dir_img_dataset):\n        for img_filename in img_files:\n            if img_filename.endswith(\".jpg\"):\n                img_path = str(os.path.join(sub_dir, img_filename))\n                img = PIL.Image.open(img_path)\n                img_col_features = get_img_colour_features(img)\n\n                img_features.append(\n                    np.append(np.array(str(os.path.join(sub_dir, img_filename))),img_col_features)\n                )\n                count+=1\n\n        print(sub_dir, count)\n\n    return pd.DataFrame(\n        data=img_features,\n        columns=[\"Path\", \"Red\",\"Green\",\"Blue\",\"Hue\",\"Sat\",\"Val\",\"Cyan\",\"Mag\",\"Yel\"] \n    )","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.404733Z","iopub.execute_input":"2023-06-28T19:26:17.405150Z","iopub.status.idle":"2023-06-28T19:26:17.412634Z","shell.execute_reply.started":"2023-06-28T19:26:17.405113Z","shell.execute_reply":"2023-06-28T19:26:17.411550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Colour feature generation\n# train_col_features = generate_col_features(train_art_dir)\n# test_col_features = generate_col_features(test_art_dir)\n\n# train_col_features[\"style\"] = train_col_features.apply(lambda df_row: df_row[\"Path\"].split(\"/\")[-2], axis=1)\n# test_col_features[\"style\"] = test_col_features.apply(lambda df_row: df_row[\"Path\"].split(\"/\")[-2], axis=1)\n# train_col_features[\"label\"] = train_col_features.apply(lambda df_row: art_class_labels.index(df_row[\"style\"]), axis=1)\n# test_col_features[\"label\"] = test_col_features.apply(lambda df_row: art_class_labels.index(df_row[\"style\"]), axis=1)\n\n# # Saving to CSV\n# train_col_features.to_csv(\"train_art_col_feat.csv\")\n# test_col_features.to_csv(\"test_art_col_feat.csv\")\n\n# raise SystemExit(\"Stopping!\")","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.414131Z","iopub.execute_input":"2023-06-28T19:26:17.414416Z","iopub.status.idle":"2023-06-28T19:26:17.424534Z","shell.execute_reply.started":"2023-06-28T19:26:17.414394Z","shell.execute_reply":"2023-06-28T19:26:17.423683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_col_features = pd.read_csv(\"/kaggle/input/real-ai-art/train_art_col_feat.csv\")\ntest_col_features = pd.read_csv(\"/kaggle/input/real-ai-art/test_art_col_feat.csv\")\ntrain_col_features.drop(\"Unnamed: 0\", inplace=True, axis=1, errors='ignore')\ntest_col_features.drop(\"Unnamed: 0\", inplace=True, axis=1, errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:17.425491Z","iopub.execute_input":"2023-06-28T19:26:17.426238Z","iopub.status.idle":"2023-06-28T19:26:18.080427Z","shell.execute_reply.started":"2023-06-28T19:26:17.426208Z","shell.execute_reply":"2023-06-28T19:26:18.079508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_col_features.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:18.082903Z","iopub.execute_input":"2023-06-28T19:26:18.083203Z","iopub.status.idle":"2023-06-28T19:26:18.105271Z","shell.execute_reply.started":"2023-06-28T19:26:18.083180Z","shell.execute_reply":"2023-06-28T19:26:18.104049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\n\ntrain_col_features[train_col_features.columns[1:10]] = scaler.fit_transform(train_col_features[train_col_features.columns[1:10]])\ntest_col_features[test_col_features.columns[1:10]] = scaler.fit_transform(test_col_features[test_col_features.columns[1:10]])","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:18.106211Z","iopub.execute_input":"2023-06-28T19:26:18.106465Z","iopub.status.idle":"2023-06-28T19:26:18.139372Z","shell.execute_reply.started":"2023-06-28T19:26:18.106446Z","shell.execute_reply":"2023-06-28T19:26:18.138636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_col_features.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:18.140382Z","iopub.execute_input":"2023-06-28T19:26:18.140620Z","iopub.status.idle":"2023-06-28T19:26:18.155445Z","shell.execute_reply.started":"2023-06-28T19:26:18.140600Z","shell.execute_reply":"2023-06-28T19:26:18.154188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code to check any dulplicate files under same name\n# count = 0\n# for sub_dir, dirs, img_files in os.walk(test_art_dir):\n#     for img_filename in img_files:\n#         if img_filename.endswith(\".jpg\"):\n#             if len(test_col_features[test_col_features['Path'].str.endswith(\"/\"+img_filename.split(\"/\")[-1])].iloc[:,1:-2].values) != 1:\n#                 print(sub_dir, img_filename)\n#             count+=1\n\n#     print(sub_dir, count)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:18.156343Z","iopub.execute_input":"2023-06-28T19:26:18.156580Z","iopub.status.idle":"2023-06-28T19:26:18.164119Z","shell.execute_reply.started":"2023-06-28T19:26:18.156562Z","shell.execute_reply":"2023-06-28T19:26:18.163310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Creation","metadata":{}},{"cell_type":"code","source":"# Make a CSV with addictional features \n# Build a class like below to integrate the features with the image and the label\n# https://medium.com/bivek-adhikari/creating-custom-datasets-and-dataloaders-with-pytorch-7e9d2f06b660\n# https://pytorch.org/vision/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n# Inherit the DatasetFolder class and overide the __get__() function\nfrom sklearn.preprocessing import StandardScaler\nfrom torchvision.datasets import DatasetFolder\nfrom torchvision.datasets.folder import default_loader, IMG_EXTENSIONS\n\nclass ArtStyleDataset(DatasetFolder):\n\n    def __init__(self, dir_root, csv_file_path, transform=None, \n                 target_transform=None, is_valid_file=None, feature_scaler=None):\n        super().__init__(\n            dir_root,\n            default_loader,\n            IMG_EXTENSIONS if is_valid_file is None else None,\n            transform=transform\n        )\n        \n        self.dir_root = dir_root\n        self.transform = transform\n        self.feature_scaler = feature_scaler\n        self.img_feat_data = self.get_preprocesseed_features(csv_file_path) # Path Red Green Blue Hue Sat Val Cyan Mag Yel style label\n\n    def __len__(self) -> int:\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        img_feature = self.img_feat_data[index]\n        \n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, img_feature, target\n\n    \n    def get_preprocesseed_features(self, csv_file_path):\n        \"\"\"\n        Preprocesses the image features\n        \"\"\"\n        print(\"Preprecessing...\")\n        \n        col_features_tensor_list = []\n        col_features = pd.read_csv(csv_file_path)\n        col_features.drop(\"Unnamed: 0\", inplace=True, axis=1, errors='ignore')\n        \n        # Scaling\n        if self.feature_scaler is None:\n            self.feature_scaler = StandardScaler().fit(col_features[col_features.columns[1:10]])\n        col_features[col_features.columns[1:10]] = self.feature_scaler.transform(col_features[col_features.columns[1:10]])\n        \n        for path, _ in tqdm(self.samples):\n            col_features_tensor_list.append(\n                torch.Tensor(\n                    col_features[col_features['Path'].str.endswith(\"/\"+path.split(\"/\")[-1])].iloc[:,1:-2].values\n                )\n            )\n        \n        print(\"Feature count: \", len(col_features_tensor_list))\n        \n        return col_features_tensor_list\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:39.465375Z","iopub.execute_input":"2023-06-28T19:26:39.465750Z","iopub.status.idle":"2023-06-28T19:26:39.476524Z","shell.execute_reply.started":"2023-06-28T19:26:39.465724Z","shell.execute_reply":"2023-06-28T19:26:39.475627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Must be done with the custom dataset to be created\nimagenet_weights = models.ConvNeXt_Base_Weights.DEFAULT\npreprocess_transforms = imagenet_weights.transforms()\npreprocess_transforms","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:44.547847Z","iopub.execute_input":"2023-06-28T19:26:44.548814Z","iopub.status.idle":"2023-06-28T19:26:44.556017Z","shell.execute_reply.started":"2023-06-28T19:26:44.548788Z","shell.execute_reply":"2023-06-28T19:26:44.554649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_data = ArtStyleDataset( # EXPERIMENT!!!!!!!!!!!!!!!!!\n    valid_art_dir,\n    \"/kaggle/input/real-ai-art/test_art_col_feat.csv\",\n    transform=preprocess_transforms\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:26:45.332214Z","iopub.execute_input":"2023-06-28T19:26:45.333219Z","iopub.status.idle":"2023-06-28T19:28:28.427283Z","shell.execute_reply.started":"2023-06-28T19:26:45.333178Z","shell.execute_reply":"2023-06-28T19:28:28.425691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open('./test_col_feature_scaler.pkl','wb') as sc_f:\n#     pickle.dump(exp_data.feature_scaler, sc_f)\n# # with open('./test_col_feature_scaler.pkl','rb') as sc_f:\n# #     sc = pickle.load(sc_f)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T19:22:44.719487Z","iopub.execute_input":"2023-06-21T19:22:44.719774Z","iopub.status.idle":"2023-06-21T19:22:44.724184Z","shell.execute_reply.started":"2023-06-21T19:22:44.719751Z","shell.execute_reply":"2023-06-21T19:22:44.723014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Means: \", exp_data.feature_scaler.mean_)\nprint(\"Var: \", exp_data.feature_scaler.var_)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T19:22:44.725411Z","iopub.execute_input":"2023-06-21T19:22:44.725713Z","iopub.status.idle":"2023-06-21T19:22:44.740078Z","shell.execute_reply.started":"2023-06-21T19:22:44.725686Z","shell.execute_reply":"2023-06-21T19:22:44.739262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image(image, img_feature, label):\n    print(f\"Label: {label}\") \n    print(f\"Feature: {img_feature}\") \n    plt.imshow(image.permute(1,2,0))\n    plt.show()\n\nshow_image(*exp_data[0])","metadata":{"execution":{"iopub.status.busy":"2023-06-21T19:22:44.741266Z","iopub.execute_input":"2023-06-21T19:22:44.741627Z","iopub.status.idle":"2023-06-21T19:22:45.206049Z","shell.execute_reply.started":"2023-06-21T19:22:44.741597Z","shell.execute_reply":"2023-06-21T19:22:45.205001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_data_dl = DataLoader(\n    exp_data,\n    batch_size=1,\n    shuffle=True,\n    num_workers=os.cpu_count()\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T19:22:45.207470Z","iopub.execute_input":"2023-06-21T19:22:45.209667Z","iopub.status.idle":"2023-06-21T19:22:45.215051Z","shell.execute_reply.started":"2023-06-21T19:22:45.209619Z","shell.execute_reply":"2023-06-21T19:22:45.213936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_WORKERS = os.cpu_count()\n\ndef get_art_image_dl(\n    train_dir: str,\n    train_csv_dir: str,\n    valid_dir: str,\n    valid_csv_dir: str,\n    test_dir: str,\n    test_csv_dir: str,\n    pin_memory: bool,\n    transform: Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n    \"\"\"\n    Dataset Generation\n    \"\"\"\n\n    print(\"Train split creation...\")\n    # Creation of DataLoaders from Datasets\n    train_data = ArtStyleDataset(\n        train_dir,\n        train_csv_dir,\n        transform=transform\n    )\n    train_data_dl = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n    \n    print(\"Validation split creation...\")\n    valid_data = ArtStyleDataset(\n        valid_dir,\n        valid_csv_dir,\n        transform=transform\n    )\n    valid_data_dl = DataLoader(\n        valid_data,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n    \n    print(\"Test split creation...\")\n    test_data = ArtStyleDataset(\n        test_dir,\n        test_csv_dir,\n        transform=transform,\n        feature_scaler=train_data.feature_scaler\n    )\n    test_data_dl = DataLoader(\n        test_data,\n        batch_size=1,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n    \n    print(\"Colour feature Means     : \", train_data.feature_scaler.mean_)\n    print(\"Colour feature Variances : \", train_data.feature_scaler.var_)\n    \n    print(\"Svaing standard scaler...\")\n    with open('./col_feature_scaler.pkl','wb') as sc_f:\n        pickle.dump(train_data.feature_scaler, sc_f)\n\n    return train_data_dl, valid_data_dl, test_data_dl, train_data.classes, train_data.feature_scaler","metadata":{"execution":{"iopub.status.busy":"2023-06-21T19:23:00.180843Z","iopub.execute_input":"2023-06-21T19:23:00.181270Z","iopub.status.idle":"2023-06-21T19:23:00.191702Z","shell.execute_reply.started":"2023-06-21T19:23:00.181239Z","shell.execute_reply":"2023-06-21T19:23:00.190496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE=32","metadata":{"execution":{"iopub.status.busy":"2023-06-21T19:23:00.698217Z","iopub.execute_input":"2023-06-21T19:23:00.698613Z","iopub.status.idle":"2023-06-21T19:23:00.702931Z","shell.execute_reply.started":"2023-06-21T19:23:00.698582Z","shell.execute_reply":"2023-06-21T19:23:00.701976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_art_dl, valid_art_dl, test_art_dl, art_style_classes, col_feature_scaler = get_art_image_dl(\n    train_dir=train_art_dir, \n    valid_dir=valid_art_dir, \n    test_dir=test_art_dir,\n    train_csv_dir=train_art_csv_dir, \n    valid_csv_dir=valid_art_csv_dir, \n    test_csv_dir=test_art_csv_dir,\n    pin_memory=True,\n    transform=preprocess_transforms,\n    batch_size=BATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T19:23:01.086359Z","iopub.execute_input":"2023-06-21T19:23:01.086798Z","iopub.status.idle":"2023-06-21T19:23:34.383360Z","shell.execute_reply.started":"2023-06-21T19:23:01.086763Z","shell.execute_reply":"2023-06-21T19:23:34.381130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train Size      : {len(train_art_dl)} | Batch Size: {BATCH_SIZE}\")\nprint(f\"Validation Size : {len(valid_art_dl)} | Batch Size: {BATCH_SIZE}\")\nprint(f\"Test Size       : {len(test_art_dl)}  | Batch Size: {1}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-21T19:23:34.385277Z","iopub.status.idle":"2023-06-21T19:23:34.386403Z","shell.execute_reply.started":"2023-06-21T19:23:34.386049Z","shell.execute_reply":"2023-06-21T19:23:34.386082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"train_art_img_batch, train_col_feat_batch, train_art_labels_batch = next(iter(train_art_dl))\ntrain_art_img_batch.shape, train_col_feat_batch.shape, train_art_labels_batch.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:25.938718Z","iopub.execute_input":"2023-06-11T16:14:25.939158Z","iopub.status.idle":"2023-06-11T16:14:30.041882Z","shell.execute_reply.started":"2023-06-11T16:14:25.939121Z","shell.execute_reply":"2023-06-11T16:14:30.040834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_filename, art_style_label = train_art_img_batch[0], train_art_labels_batch[0]","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:30.045047Z","iopub.execute_input":"2023-06-11T16:14:30.045578Z","iopub.status.idle":"2023-06-11T16:14:30.050220Z","shell.execute_reply.started":"2023-06-11T16:14:30.045547Z","shell.execute_reply":"2023-06-11T16:14:30.049228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_ch, img_h, img_w = img_filename.shape\nimg_ch, img_h, img_w","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:30.051761Z","iopub.execute_input":"2023-06-11T16:14:30.052467Z","iopub.status.idle":"2023-06-11T16:14:30.062622Z","shell.execute_reply.started":"2023-06-11T16:14:30.052427Z","shell.execute_reply":"2023-06-11T16:14:30.061728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"art_style_label","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:30.064079Z","iopub.execute_input":"2023-06-11T16:14:30.064944Z","iopub.status.idle":"2023-06-11T16:14:30.073941Z","shell.execute_reply.started":"2023-06-11T16:14:30.064910Z","shell.execute_reply":"2023-06-11T16:14:30.073109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"art_style_classes","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:30.075665Z","iopub.execute_input":"2023-06-11T16:14:30.076330Z","iopub.status.idle":"2023-06-11T16:14:30.085680Z","shell.execute_reply.started":"2023-06-11T16:14:30.076299Z","shell.execute_reply":"2023-06-11T16:14:30.084701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fg = plt.figure(figsize=(9, 4))\nrows, cols = 2, 4\nfor i in range(1, rows * cols + 1):\n    rand_art_idx = torch.randint(0, len(train_art_img_batch), size=[1]).item()\n    art_img, art_style_label = train_art_img_batch[rand_art_idx], train_art_labels_batch[rand_art_idx]\n    fg.add_subplot(rows, cols, i)\n    plt.imshow(art_img.squeeze().permute(1, 2, 0), cmap=\"gray\")\n    plt.title(art_style_classes[art_style_label])\n    plt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:30.087324Z","iopub.execute_input":"2023-06-11T16:14:30.088059Z","iopub.status.idle":"2023-06-11T16:14:30.847526Z","shell.execute_reply.started":"2023-06-11T16:14:30.088026Z","shell.execute_reply":"2023-06-11T16:14:30.846698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Development","metadata":{}},{"cell_type":"markdown","source":"Potential improvements\n- Colour percentages\n- Egdes","metadata":{}},{"cell_type":"code","source":"from torchvision.models import convnext_base\nfrom torchvision.models import ConvNeXt_Base_Weights\nclass ArtVisionModel(nn.Module):\n    \n    def __init__(self, n_classes, n_art_img_features=9):\n        super(ArtVisionModel, self).__init__()\n        \n        # Pretrained ConvNeXt\n        self.convnext = convnext_base(weights=ConvNeXt_Base_Weights.DEFAULT)\n        \n        # Freezing top params of the model\n        for train_param in self.convnext.features[:-1].parameters():\n            train_param.requires_grad = False\n        \n        # Features\n        self.features = self.convnext.features\n        \n        self.features_low_CNB = self.convnext.features[:4]\n        \n        # Features - Low level\n        self.features_low = self.convnext.features[4]\n        \n        # Features - Mid level\n        self.features_mid_CNB = self.convnext.features[5]\n        \n        # Features - High level\n        self.features_mid = self.convnext.features[6]\n        \n        # Features - Top level\n        self.features_high_CNB = self.convnext.features[7]\n    \n        self.mid_feat_pooling = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten()\n        )\n        \n        # CNBlock pooling layer\n        self.high_feat_pooling = nn.Sequential(\n            self.convnext.avgpool,\n            self.convnext.classifier[0],\n            nn.Flatten()\n        )\n        \n                \n        # Colour features\n        self.col_features = nn.Sequential(\n            nn.Linear(\n                in_features=n_art_img_features,\n                out_features=64,\n                bias=True\n            ),\n            nn.Flatten()\n        )\n\n        # Classifier portion\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.4), \n            nn.Linear(\n                in_features=2560+64,\n                out_features=512,\n                bias=True\n            ),\n            nn.GELU(),\n            nn.Dropout(p=0.1),\n            nn.Linear(\n                in_features=512,\n                out_features=n_classes,\n                bias=True\n            )\n        )\n    \n    def forward(self, img, img_col_feat):\n        # Features\n        x = self.features_low_CNB(img)\n        x_low = self.features_low(x)\n        \n        x = self.features_mid_CNB(x_low)\n        x_mid = self.features_mid(x)\n        \n        x = self.features_high_CNB(x_mid)\n\n        # Pooling\n        x_high = self.high_feat_pooling(x)\n        x_mid = self.mid_feat_pooling(x_mid)\n        x_low = self.mid_feat_pooling(x_low)\n        \n        # Colour features\n        img_col_feat = self.col_features(img_col_feat)\n\n        # Feature concatenation\n        x = torch.cat((x_high, x_mid, x_low, img_col_feat), dim=1)\n                        \n        # Classifier\n        x = self.classifier(x)\n        \n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:02:01.287785Z","iopub.execute_input":"2023-06-28T19:02:01.288233Z","iopub.status.idle":"2023-06-28T19:02:01.305354Z","shell.execute_reply.started":"2023-06-28T19:02:01.288200Z","shell.execute_reply":"2023-06-28T19:02:01.304477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# art_vision_model = torch.load(\"/kaggle/input/art-model-pytorch/art_brain_model.pt\", map_location=device)\n# model_scripted = torch.jit.script(art_vision_model)# Export to TorchScript\n\n# model_scripted.save('art_brain_model_scripted.pt') # Save\n#torch.save(art_vision_model.state_dict(), \"./art_vision_model_state.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:02:02.530747Z","iopub.execute_input":"2023-06-28T19:02:02.531426Z","iopub.status.idle":"2023-06-28T19:02:02.535214Z","shell.execute_reply.started":"2023-06-28T19:02:02.531394Z","shell.execute_reply":"2023-06-28T19:02:02.534489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"art_vision_model = ArtVisionModel(n_classes=len(art_style_classes)).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:02:10.831339Z","iopub.execute_input":"2023-06-28T19:02:10.831741Z","iopub.status.idle":"2023-06-28T19:02:13.036760Z","shell.execute_reply.started":"2023-06-28T19:02:10.831713Z","shell.execute_reply":"2023-06-28T19:02:13.035590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"art_vision_model.features[-1][-1]","metadata":{"execution":{"iopub.status.busy":"2023-06-28T18:55:45.876880Z","iopub.execute_input":"2023-06-28T18:55:45.877282Z","iopub.status.idle":"2023-06-28T18:55:45.884911Z","shell.execute_reply.started":"2023-06-28T18:55:45.877238Z","shell.execute_reply":"2023-06-28T18:55:45.883922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-28T18:55:52.878083Z","iopub.execute_input":"2023-06-28T18:55:52.879228Z","iopub.status.idle":"2023-06-28T18:55:52.888217Z","shell.execute_reply.started":"2023-06-28T18:55:52.879185Z","shell.execute_reply":"2023-06-28T18:55:52.886867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torchinfo.summary(\n    model=art_vision_model, \n    input_size=[(32, 3, img_h, img_w), (32, 9)],\n    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n    col_width=20,\n    row_settings=[\"var_names\"]\n) ","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:02:32.127909Z","iopub.execute_input":"2023-06-28T19:02:32.128394Z","iopub.status.idle":"2023-06-28T19:02:41.949311Z","shell.execute_reply.started":"2023-06-28T19:02:32.128359Z","shell.execute_reply":"2023-06-28T19:02:41.948203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchview import draw_graph\n\nmodel_graph = draw_graph(art_vision_model, input_size=[(32, 3, img_h, img_w), (32, 9)], expand_nested=True)\nmodel_graph.visual_graph","metadata":{"execution":{"iopub.status.busy":"2023-06-28T19:02:41.951431Z","iopub.execute_input":"2023-06-28T19:02:41.952526Z","iopub.status.idle":"2023-06-28T19:02:52.180376Z","shell.execute_reply.started":"2023-06-28T19:02:41.952487Z","shell.execute_reply":"2023-06-28T19:02:52.179301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#art_vision_model = nn.DataParallel(art_vision_model)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:42.901283Z","iopub.execute_input":"2023-06-11T16:14:42.902169Z","iopub.status.idle":"2023-06-11T16:14:42.907351Z","shell.execute_reply.started":"2023-06-11T16:14:42.902136Z","shell.execute_reply":"2023-06-11T16:14:42.905609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(art_vision_model.parameters(), lr=0.001)\nlr_scheduler = ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.1, patience=2, \n    threshold=0.0001, cooldown=0, min_lr=1e-06, verbose=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:07:40.124597Z","iopub.execute_input":"2023-06-14T08:07:40.126043Z","iopub.status.idle":"2023-06-14T08:07:40.136938Z","shell.execute_reply.started":"2023-06-14T08:07:40.125990Z","shell.execute_reply":"2023-06-14T08:07:40.135719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\n\nfrom typing import Dict, List, Tuple\n\ndef train_step_procedure(\n    epoch: int,\n    model: torch.nn.Module, \n    train_dl: torch.utils.data.DataLoader, \n    loss_fn: torch.nn.Module, \n    optimizer: torch.optim.Optimizer,\n    device: torch.device) -> Tuple[float, float]:\n    \"\"\"\n    \n    \"\"\"\n    model.train()\n\n    train_step_loss, train_step_acc = 0, 0\n    \n    training_pbar = tqdm(enumerate(train_dl), unit=\"batch\", total=len(train_dl))\n    training_pbar.set_description(\"Epoch %02d\" % (epoch+1))\n\n    for batch, (train_image, train_feat, img_label) in training_pbar:\n        \n        train_image, train_feat, img_label = train_image.to(device), train_feat.to(device), img_label.to(device)\n\n        # Forward propagation\n        y_pred = model(train_image, train_feat)\n\n        # Loss calcluation\n        loss = loss_fn(y_pred, img_label)\n        train_step_loss += loss.item() \n        avg_train_step_loss = train_step_loss/(batch+1)\n        training_pbar.set_postfix({'Loss': \"%.4f\" % avg_train_step_loss})\n\n        # Resetting optimizer\n        optimizer.zero_grad()\n\n        # Backward propagation\n        loss.backward()\n\n        # Weight optimisation\n        optimizer.step()\n\n        # Per batch Accuracy calculation\n        y_pred_class = torch.argmax(y_pred, dim=1)\n        train_step_acc += (y_pred_class == img_label).sum().item()/len(y_pred)\n\n    # Epoch train loss and accuracy\n    train_step_loss /= len(train_dl)\n    train_step_acc /= len(train_dl)\n    return train_step_loss, train_step_acc\n\ndef validation_step_procedure(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n    \"\"\"\n    \n    \"\"\"\n    # Evaluation mode activation\n    model.eval() \n\n    # Setup test loss and test accuracy values\n    valid_step_loss, valid_step_acc = 0, 0\n\n    # Torch inference context manager for efficient performance\n    with torch.inference_mode():\n        for batch, (valid_image, valid_feat, valid_label) in enumerate(dataloader):\n            valid_image, valid_feat, valid_label = valid_image.to(device), valid_feat.to(device), valid_label.to(device)\n\n            valid_pred = model(valid_image, valid_feat)\n\n            # Batch loss\n            loss = loss_fn(valid_pred, valid_label)\n            valid_step_loss += loss.item()\n\n            # Batch accuracy\n            valid_pred_labels = valid_pred.argmax(dim=1)\n            valid_step_acc += ((valid_pred_labels == valid_label).sum().item()/len(valid_pred_labels))\n\n    valid_step_loss = valid_step_loss / len(dataloader)\n    valid_step_acc = valid_step_acc / len(dataloader)\n    return valid_step_loss, valid_step_acc\n\ndef train(\n    model: torch.nn.Module, \n    train_dl: torch.utils.data.DataLoader, \n    validation_dl: torch.utils.data.DataLoader, \n    optimizer: torch.optim.Optimizer,\n    loss_fn: torch.nn.Module,\n    epochs: int,\n    lr_scheduler: torch.optim.lr_scheduler,\n    device: torch.device\n) -> Dict[str, List]:\n    \"\"\"\n    \n    \"\"\"\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"valid_loss\": [],\n               \"valid_acc\": [],\n               \"lr\": []\n    }\n    \n    model.to(device)\n\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step_procedure(\n            epoch=epoch,\n            model=model,\n            train_dl=train_dl,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n            device=device\n        )\n        \n        valid_loss, valid_acc = validation_step_procedure(\n            model=model,\n            dataloader=validation_dl,\n            loss_fn=loss_fn,\n            device=device\n        )\n        \n                \n        # Learning rate adjustments\n        lr_scheduler.step(valid_loss)\n\n        print(\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"valid_loss: {valid_loss:.4f} | \"\n          f\"valid_acc: {valid_acc:.4f} | \"\n          f\"lr: {optimizer.param_groups[0]['lr']}\"\n        )\n\n        # Updating results\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"valid_loss\"].append(valid_loss)\n        results[\"valid_acc\"].append(valid_acc)\n        results[\"lr\"].append(optimizer.param_groups[0]['lr'])\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:42.929356Z","iopub.execute_input":"2023-06-11T16:14:42.931812Z","iopub.status.idle":"2023-06-11T16:14:42.961762Z","shell.execute_reply.started":"2023-06-11T16:14:42.931779Z","shell.execute_reply":"2023-06-11T16:14:42.960766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.randn(3, 5, requires_grad=True), torch.empty(3, dtype=torch.long).random_(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:42.966814Z","iopub.execute_input":"2023-06-11T16:14:42.969434Z","iopub.status.idle":"2023-06-11T16:14:42.974781Z","shell.execute_reply.started":"2023-06-11T16:14:42.969401Z","shell.execute_reply":"2023-06-11T16:14:42.973809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 15","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:42.979716Z","iopub.execute_input":"2023-06-11T16:14:42.982088Z","iopub.status.idle":"2023-06-11T16:14:42.987681Z","shell.execute_reply.started":"2023-06-11T16:14:42.982056Z","shell.execute_reply":"2023-06-11T16:14:42.986697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_restults = train(\n    model=art_vision_model, \n    train_dl=train_art_dl, \n    validation_dl=valid_art_dl, \n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    lr_scheduler=lr_scheduler,\n    epochs=epochs,\n    device=device\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:14:42.990796Z","iopub.execute_input":"2023-06-11T16:14:42.991599Z","iopub.status.idle":"2023-06-11T16:24:30.260309Z","shell.execute_reply.started":"2023-06-11T16:14:42.991567Z","shell.execute_reply":"2023-06-11T16:24:30.257799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training results","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:24:30.261240Z","iopub.status.idle":"2023-06-11T16:24:30.261606Z","shell.execute_reply.started":"2023-06-11T16:24:30.261436Z","shell.execute_reply":"2023-06-11T16:24:30.261453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:24:39.921603Z","iopub.execute_input":"2023-06-11T16:24:39.922002Z","iopub.status.idle":"2023-06-11T16:24:40.441909Z","shell.execute_reply.started":"2023-06-11T16:24:39.921968Z","shell.execute_reply":"2023-06-11T16:24:40.440862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## art_vision_model = torch.load(\"/kaggle/input/art-model-pytorch/art_brain_model.pt\", map_location=device)\n## model_scripted = torch.jit.script(art_vision_model)# Export to TorchScript\n\n## model_scripted.save('art_brain_model_scripted.pt') # Save","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:03:07.362035Z","iopub.execute_input":"2023-06-14T08:03:07.362408Z","iopub.status.idle":"2023-06-14T08:03:13.319072Z","shell.execute_reply.started":"2023-06-14T08:03:07.362379Z","shell.execute_reply":"2023-06-14T08:03:13.318072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_attribution_pred(pred):\n    \n    ld_score = np.sum(pred[0:10])\n    sd_score = np.sum(pred[10:20])\n    real_score = np.sum(pred[20:])\n    \n    attr_preds = [ld_score, sd_score, real_score]\n    \n    return attr_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_attribution_label(sub_label):\n    return math.floor(sub_label/10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nlabels = []\n\nattr_preds = []\nattr_labels = []\n\nart_vision_model.eval()\nwith torch.inference_mode():\n    # Loop through DataLoader batches\n    for batch, (test_image, test_feat, test_label) in tqdm(\n        enumerate(testing_dl), unit=\"images\", total=len(testing_dl)\n    ):\n        \n        # Send data to target device\n        test_image, test_feat, test_label = test_image.to(device), test_feat.to(device), test_label.to(device)\n\n        # Batch Inference\n        test_pred = art_vision_model(test_image, test_feat)\n\n        # Batch loss\n        loss = loss_fn(test_pred, test_label)\n\n        # Batch accuracy\n        test_pred_labels = test_pred.argmax(dim=1)\n        \n        preds.append(test_pred_labels[0].item())\n        labels.append(test_label[0].item())\n        \n        test_pred_np = F.softmax(test_pred.detach(), dim=1).cpu().numpy()[0] # TODO: Try softmax here\n        \n        attr_preds.append(\n            np.argmax(get_attribution_pred(test_pred_np))\n        )\n        \n        attr_labels.append(\n            get_attribution_label(test_label[0].item())\n        )\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:07:53.513997Z","iopub.execute_input":"2023-06-14T08:07:53.514414Z","iopub.status.idle":"2023-06-14T08:10:52.533217Z","shell.execute_reply.started":"2023-06-14T08:07:53.514381Z","shell.execute_reply":"2023-06-14T08:10:52.531980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(preds), len(labels)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:11:31.864908Z","iopub.execute_input":"2023-06-14T08:11:31.865577Z","iopub.status.idle":"2023-06-14T08:11:31.881347Z","shell.execute_reply.started":"2023-06-14T08:11:31.865530Z","shell.execute_reply":"2023-06-14T08:11:31.880241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds[:10], labels[:10]","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:11:32.022198Z","iopub.execute_input":"2023-06-14T08:11:32.022708Z","iopub.status.idle":"2023-06-14T08:11:32.031594Z","shell.execute_reply.started":"2023-06-14T08:11:32.022619Z","shell.execute_reply":"2023-06-14T08:11:32.030408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attr_preds[:10], attr_labels[:10]","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:11:32.175280Z","iopub.execute_input":"2023-06-14T08:11:32.175855Z","iopub.status.idle":"2023-06-14T08:11:32.184692Z","shell.execute_reply.started":"2023-06-14T08:11:32.175811Z","shell.execute_reply":"2023-06-14T08:11:32.183482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(labels, preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=art_style_classes)\n\nfig, ax = plt.subplots(figsize=(15,15))\ndisp.plot(ax=ax)\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:12:03.990640Z","iopub.execute_input":"2023-06-14T08:12:03.991080Z","iopub.status.idle":"2023-06-14T08:12:04.723742Z","shell.execute_reply.started":"2023-06-14T08:12:03.991045Z","shell.execute_reply":"2023-06-14T08:12:04.722562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_models = [\"standard_diffusion\", \"latent_diffusion\", \"human\"]\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(attr_labels, attr_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gen_models)\n\ndisp.plot()\nplt.xticks(rotation=90)   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ART_MODEL_PATH = \"./art_classifier.pt\"\ntorch.save(art_vision_model.state_dict(), ART_MODEL_PATH)\n#torch.save(art_vision_model.state_dict(), \"./art_vision_model_state.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:12:22.544596Z","iopub.execute_input":"2023-06-14T08:12:22.545299Z","iopub.status.idle":"2023-06-14T08:12:23.140864Z","shell.execute_reply.started":"2023-06-14T08:12:22.545261Z","shell.execute_reply":"2023-06-14T08:12:23.139773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GradCAM","metadata":{}},{"cell_type":"code","source":"def get_img_colour_features(image, col_feature_scaler):\n    rgb_img_features = np.array(image.resize((1,1))).squeeze()\n    hsv_img_features = np.array(image.convert('HSV').resize((1,1))).squeeze()\n    cmyk_img_features = np.array(image.convert('CMYK').resize((1,1))).squeeze()[:-1]\n    \n    return torch.Tensor(col_feature_scaler.transform(\n        [np.concatenate((rgb_img_features, hsv_img_features, cmyk_img_features))]\n    ))","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:33.992697Z","iopub.execute_input":"2023-06-14T08:19:33.993447Z","iopub.status.idle":"2023-06-14T08:19:34.000672Z","shell.execute_reply.started":"2023-06-14T08:19:33.993408Z","shell.execute_reply":"2023-06-14T08:19:33.999370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#art_vision_model = torch.load(\"/kaggle/input/art-model-pytorch/art_classifier.pt\", map_location=device).module","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:34.357521Z","iopub.execute_input":"2023-06-14T08:19:34.358685Z","iopub.status.idle":"2023-06-14T08:19:34.364155Z","shell.execute_reply.started":"2023-06-14T08:19:34.358622Z","shell.execute_reply":"2023-06-14T08:19:34.362612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_pred(model, art_img_tensor, art_img_col_features):\n    for train_param in art_vision_model.features.parameters(): # TODO: For all layers of just for the features?\n        train_param.requires_grad = True\n    \n    gradients = None\n    activations = None\n\n    def hook_backward(module, grad_input, grad_output):\n        nonlocal gradients\n        gradients = grad_output\n\n    def hook_forward(module, args, output):\n        nonlocal activations\n        activations = output\n        \n# art_vision_model.features[-1][-1]       \n#\n#     CNBlock(\n#       (block): Sequential(\n#         (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024) | Size [1, 1024, 7, 7]\n#         (1): Permute()\n#         (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n#         (3): Linear(in_features=1024, out_features=4096, bias=True)\n#         (4): GELU(approximate='none')\n#         (5): Linear(in_features=4096, out_features=1024, bias=True)\n#         (6): Permute()\n#       )\n#       (stochastic_depth): StochasticDepth(p=0.5, mode=row)\n#     )\n        \n    hook_backward = model.features[-1][-1].block[0].register_full_backward_hook(hook_backward, prepend=False)\n    hook_forward = model.features[-1][-1].block[0].register_forward_hook(hook_forward, prepend=False)\n    \n    model.eval()\n    \n    preds =  model(art_img_tensor.unsqueeze(0), art_img_col_features)\n    pred_index = preds.argmax(dim=1)\n\n    preds[:, pred_index].backward()\n    \n    hook_backward.remove()\n    hook_forward.remove()\n    \n    for train_param in art_vision_model.features.parameters():\n        train_param.requires_grad = False\n    \n    return pred_index, gradients, activations\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:35.340218Z","iopub.execute_input":"2023-06-14T08:19:35.340988Z","iopub.status.idle":"2023-06-14T08:19:35.352391Z","shell.execute_reply.started":"2023-06-14T08:19:35.340952Z","shell.execute_reply":"2023-06-14T08:19:35.351264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_grad_map(gradients, activations):\n\n    avg_pooled_gradients = torch.mean(\n        gradients[0], # Size [1, 1024, 7, 7]\n        dim=[0, 2, 3]\n    )\n\n    # Weighting acitvation features (channels) using its related calculated Gradient\n    for i in range(activations.size()[1]):\n        activations[:, i, :, :] *= avg_pooled_gradients[i]\n\n    # average the channels of the activations\n    heatmap = torch.mean(activations, dim=1).squeeze()\n    \n    # L2 Normalisation # IMPROVED GRADCAM!!!!! EXPERIMENT MORE\n    heatmap = F.normalize(heatmap)\n        \n    # relu on top of the heatmap\n    heatmap = F.sigmoid(heatmap)\n\n    # Min-max normalization of the heatmap\n    heatmap = (heatmap - torch.min(heatmap))/(torch.max(heatmap) - torch.min(heatmap))\n\n    return heatmap.detach()","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:35.529356Z","iopub.execute_input":"2023-06-14T08:19:35.529787Z","iopub.status.idle":"2023-06-14T08:19:35.538767Z","shell.execute_reply.started":"2023-06-14T08:19:35.529752Z","shell.execute_reply":"2023-06-14T08:19:35.537548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_art_img, _, test_art_labels = next(iter(valid_art_dl))","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:36.053943Z","iopub.execute_input":"2023-06-14T08:19:36.054373Z","iopub.status.idle":"2023-06-14T08:19:36.098539Z","shell.execute_reply.started":"2023-06-14T08:19:36.054339Z","shell.execute_reply":"2023-06-14T08:19:36.096445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.matshow(test_art_img[6].permute(1,2,0))\ntest_art_labels[6]","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:36.512391Z","iopub.execute_input":"2023-06-14T08:19:36.512811Z","iopub.status.idle":"2023-06-14T08:19:36.556027Z","shell.execute_reply.started":"2023-06-14T08:19:36.512776Z","shell.execute_reply":"2023-06-14T08:19:36.554523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_col_features = get_img_colour_features(to_pil_image(test_art_img[6].detach()), col_feature_scaler)\npred_index, gradients, activations = get_model_pred(art_vision_model, test_art_img[6].to(device), img_col_features.to(device))\npred_index","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:36.729731Z","iopub.execute_input":"2023-06-14T08:19:36.732531Z","iopub.status.idle":"2023-06-14T08:19:36.776017Z","shell.execute_reply.started":"2023-06-14T08:19:36.732492Z","shell.execute_reply":"2023-06-14T08:19:36.773904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heatmap = generate_grad_map(gradients, activations)\nprint(heatmap)\nplt.matshow(heatmap.cpu())","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:38.041284Z","iopub.execute_input":"2023-06-14T08:19:38.041711Z","iopub.status.idle":"2023-06-14T08:19:38.083956Z","shell.execute_reply.started":"2023-06-14T08:19:38.041670Z","shell.execute_reply":"2023-06-14T08:19:38.082691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image(art_img, model, col_feature_scaler, hm_opacity=0.3):\n    art_img = art_img.resize((img_h,img_h), resample=PIL.Image.BICUBIC)\n    img_col_features = get_img_colour_features(art_img, col_feature_scaler)\n    art_img_tensor = preprocess_transforms(art_img)\n        \n    pred_index, gradients, activations = get_model_pred(model, art_img_tensor.to(device), img_col_features.to(device))\n    heatmap = generate_grad_map(gradients, activations)\n        \n    hm_overlay = to_pil_image(heatmap.detach().cpu(), mode='F').resize((img_h,img_h), resample=PIL.Image.BICUBIC)\n\n    # Jet Colormap\n    col_map = colormaps['YlOrRd']\n    hm_overlay = PIL.Image.fromarray(\n        (255 * col_map(np.asarray(hm_overlay) ** 2)[:, :, :3]).astype(np.uint8)\n    )\n    \n    super_impossed_img = PIL.Image.blend(art_img, hm_overlay, alpha=hm_opacity)\n    \n    return pred_index, super_impossed_img","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:38.999008Z","iopub.execute_input":"2023-06-14T08:19:39.000008Z","iopub.status.idle":"2023-06-14T08:19:39.014771Z","shell.execute_reply.started":"2023-06-14T08:19:38.999962Z","shell.execute_reply":"2023-06-14T08:19:39.013531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#art_image = PIL.Image.open(\"/kaggle/input/artbench-10-256px/artbench-10-imagefolder-split/artbench-10-imagefolder-split/test/art_nouveau/achille-beltrame_fly-of-gabriele-dannunzio-over-trieste-1915.jpg\")\n#art_image = PIL.Image.open(\"/kaggle/input/artbench-10-256px/artbench-10-imagefolder-split/artbench-10-imagefolder-split/test/romanticism/albert-bierstadt_sentinel-falls-and-cathedral-peaks-in-the-yosemite-valley-1864.jpg\")\n#art_image = PIL.Image.open(\"/kaggle/input/artbench-10-256px/artbench-10-imagefolder-split/artbench-10-imagefolder-split/test/realism/abbott-handerson-thayer_azores.jpg\")\nart_image = PIL.Image.open(\"/kaggle/input/artbench-10-256px/artbench-10-imagefolder-split/artbench-10-imagefolder-split/test/ukiyo_e/chokosai-eisho_99.jpg\")\npred_index, pred_hm = predict_image(art_image, art_vision_model, col_feature_scaler, hm_opacity=0.4)\nprint(art_style_classes[pred_index])\nplt.imshow(pred_hm)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T08:19:39.676143Z","iopub.execute_input":"2023-06-14T08:19:39.676540Z","iopub.status.idle":"2023-06-14T08:19:39.718966Z","shell.execute_reply.started":"2023-06-14T08:19:39.676507Z","shell.execute_reply":"2023-06-14T08:19:39.717370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Modularise the code. Check Notion\n# - Breakdown to classes\n# - Saving best model function implementation\n# - example: https://debuggercafe.com/saving-and-loading-the-best-model-in-pytorch/","metadata":{"execution":{"iopub.status.busy":"2023-06-11T16:28:23.098244Z","iopub.status.idle":"2023-06-11T16:28:23.099014Z","shell.execute_reply.started":"2023-06-11T16:28:23.098751Z","shell.execute_reply":"2023-06-11T16:28:23.098775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}